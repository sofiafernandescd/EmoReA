{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10da075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 14:03:32.151653: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.13)\n",
      "Loaded 7102 samples from ISEAR dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1297</th>\n",
       "      <td>21794</td>\n",
       "      <td>fear</td>\n",
       "      <td>Ffs dreadful defending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4679</th>\n",
       "      <td>20333</td>\n",
       "      <td>fear</td>\n",
       "      <td>@markhberman2003 @LanceZierlein @790blessing e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559</th>\n",
       "      <td>40950</td>\n",
       "      <td>sadness</td>\n",
       "      <td>@Magrove86Mark @clareftballnews jersey gloves?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>11104</td>\n",
       "      <td>anger</td>\n",
       "      <td>@inthefade going back to blissful ignorance?!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5661</th>\n",
       "      <td>30168</td>\n",
       "      <td>joy</td>\n",
       "      <td>@followAdamA looking back on recent tweets see...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id    label                                               text\n",
       "1297  21794     fear                             Ffs dreadful defending\n",
       "4679  20333     fear  @markhberman2003 @LanceZierlein @790blessing e...\n",
       "2559  40950  sadness  @Magrove86Mark @clareftballnews jersey gloves?...\n",
       "163   11104    anger     @inthefade going back to blissful ignorance?! \n",
       "5661  30168      joy  @followAdamA looking back on recent tweets see..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (AutoTokenizer, AutoConfig, AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding, TrainingArguments, Trainer)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import os, sys, json\n",
    "module_path = os.path.abspath(os.path.join('..', '..')) # or the path to your source code\n",
    "sys.path.insert(0, module_path)\n",
    "from src.utils import load_emorynlp, load_isear\n",
    "\n",
    "# ===========\n",
    "# 1) load dataframe\n",
    "#train_df = load_emorynlp(split='train')\n",
    "#val_df = load_emorynlp(split='dev')\n",
    "#test_df = load_emorynlp(split='test')\n",
    "#df = load_isear()\n",
    "df = pd.read_csv(\"hf://datasets/gsri-18/ISEAR-dataset-complete/ISEAR_dataset_complete.csv\")\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['label'])\n",
    "val_df, test_df = train_test_split(val_df, test_size=0.5, random_state=42, stratify=val_df['label'])\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e594492b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: {0: 'anger', 1: 'fear', 2: 'joy', 3: 'sadness'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a45c1515eca47e4a79a285f5eccbf4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4971 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942e399f1b794e788f47ae34a783e4e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1065 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5269c5b84078490896c470b93c2140f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/sofiafernandes/miniconda3/envs/varm/lib/python3.8/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/0s/lm29041s6hv5pq_lc3jblq540000gn/T/ipykernel_66858/883302262.py:79: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc5ecd1ff2b44d06ab6ece39f8b9efa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4256, 'grad_norm': 4.1073503494262695, 'learning_rate': 0.00048660235798499466, 'epoch': 0.08}\n",
      "{'loss': 1.4245, 'grad_norm': 3.7308976650238037, 'learning_rate': 0.0004732047159699893, 'epoch': 0.16}\n",
      "{'loss': 1.393, 'grad_norm': 3.5613629817962646, 'learning_rate': 0.00045980707395498397, 'epoch': 0.24}\n",
      "{'loss': 1.4177, 'grad_norm': 2.5845584869384766, 'learning_rate': 0.00044640943193997856, 'epoch': 0.32}\n",
      "{'loss': 1.3927, 'grad_norm': 2.9472012519836426, 'learning_rate': 0.0004330117899249732, 'epoch': 0.4}\n",
      "{'loss': 1.3901, 'grad_norm': 2.973587989807129, 'learning_rate': 0.00041961414790996787, 'epoch': 0.48}\n",
      "{'loss': 1.411, 'grad_norm': 2.1490581035614014, 'learning_rate': 0.0004062165058949625, 'epoch': 0.56}\n",
      "{'loss': 1.3915, 'grad_norm': 2.877133846282959, 'learning_rate': 0.0003928188638799571, 'epoch': 0.64}\n",
      "{'loss': 1.384, 'grad_norm': 1.8343064785003662, 'learning_rate': 0.00037942122186495177, 'epoch': 0.72}\n",
      "{'loss': 1.4001, 'grad_norm': 2.6202657222747803, 'learning_rate': 0.0003660235798499464, 'epoch': 0.8}\n",
      "{'loss': 1.3917, 'grad_norm': 1.2043839693069458, 'learning_rate': 0.0003526259378349411, 'epoch': 0.88}\n",
      "{'loss': 1.3823, 'grad_norm': 1.0487068891525269, 'learning_rate': 0.00033922829581993567, 'epoch': 0.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f6b0273fa7439db1e02e6b7d939661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3821744918823242, 'eval_accuracy': 0.3173708920187793, 'eval_f1_macro': 0.12045616535994298, 'eval_runtime': 66.6407, 'eval_samples_per_second': 15.981, 'eval_steps_per_second': 1.005, 'epoch': 1.0}\n",
      "{'loss': 1.4054, 'grad_norm': 1.8488689661026, 'learning_rate': 0.0003258306538049303, 'epoch': 1.05}\n",
      "{'loss': 1.3872, 'grad_norm': 1.5437819957733154, 'learning_rate': 0.00031243301178992503, 'epoch': 1.13}\n",
      "{'loss': 1.3745, 'grad_norm': 1.076107382774353, 'learning_rate': 0.0002990353697749196, 'epoch': 1.21}\n",
      "{'loss': 1.3693, 'grad_norm': 0.9571390151977539, 'learning_rate': 0.0002856377277599143, 'epoch': 1.29}\n",
      "{'loss': 1.3816, 'grad_norm': 2.0335021018981934, 'learning_rate': 0.0002722400857449089, 'epoch': 1.37}\n",
      "{'loss': 1.389, 'grad_norm': 1.1628592014312744, 'learning_rate': 0.0002588424437299036, 'epoch': 1.45}\n",
      "{'loss': 1.3863, 'grad_norm': 0.8662708401679993, 'learning_rate': 0.0002454448017148982, 'epoch': 1.53}\n",
      "{'loss': 1.3844, 'grad_norm': 0.9215345978736877, 'learning_rate': 0.0002320471596998928, 'epoch': 1.61}\n",
      "{'loss': 1.3565, 'grad_norm': 1.9284244775772095, 'learning_rate': 0.00021864951768488748, 'epoch': 1.69}\n",
      "{'loss': 1.3936, 'grad_norm': 2.388230562210083, 'learning_rate': 0.0002052518756698821, 'epoch': 1.77}\n",
      "{'loss': 1.3673, 'grad_norm': 2.6946349143981934, 'learning_rate': 0.00019185423365487676, 'epoch': 1.85}\n",
      "{'loss': 1.3826, 'grad_norm': 1.6310452222824097, 'learning_rate': 0.00017845659163987139, 'epoch': 1.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbdd63f41daa434c92655114785453d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3764488697052002, 'eval_accuracy': 0.3173708920187793, 'eval_f1_macro': 0.12045616535994298, 'eval_runtime': 68.5075, 'eval_samples_per_second': 15.546, 'eval_steps_per_second': 0.978, 'epoch': 2.0}\n",
      "{'loss': 1.3805, 'grad_norm': 1.47127103805542, 'learning_rate': 0.00016505894962486604, 'epoch': 2.01}\n",
      "{'loss': 1.3717, 'grad_norm': 1.474887728691101, 'learning_rate': 0.00015166130760986066, 'epoch': 2.09}\n",
      "{'loss': 1.3835, 'grad_norm': 1.297361969947815, 'learning_rate': 0.00013826366559485531, 'epoch': 2.17}\n",
      "{'loss': 1.3746, 'grad_norm': 0.9128406047821045, 'learning_rate': 0.00012486602357984997, 'epoch': 2.25}\n",
      "{'loss': 1.3754, 'grad_norm': 1.133837342262268, 'learning_rate': 0.00011146838156484459, 'epoch': 2.33}\n",
      "{'loss': 1.3722, 'grad_norm': 1.0803107023239136, 'learning_rate': 9.807073954983923e-05, 'epoch': 2.41}\n",
      "{'loss': 1.3696, 'grad_norm': 0.7425651550292969, 'learning_rate': 8.467309753483388e-05, 'epoch': 2.49}\n",
      "{'loss': 1.3837, 'grad_norm': 1.3321400880813599, 'learning_rate': 7.127545551982852e-05, 'epoch': 2.57}\n",
      "{'loss': 1.383, 'grad_norm': 0.967177152633667, 'learning_rate': 5.787781350482315e-05, 'epoch': 2.65}\n",
      "{'loss': 1.3738, 'grad_norm': 0.9710975289344788, 'learning_rate': 4.448017148981779e-05, 'epoch': 2.73}\n",
      "{'loss': 1.3789, 'grad_norm': 0.6366135478019714, 'learning_rate': 3.1082529474812435e-05, 'epoch': 2.81}\n",
      "{'loss': 1.3772, 'grad_norm': 1.2511193752288818, 'learning_rate': 1.7684887459807074e-05, 'epoch': 2.89}\n",
      "{'loss': 1.3716, 'grad_norm': 0.9153536558151245, 'learning_rate': 4.287245444801715e-06, 'epoch': 2.97}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:595] . unexpected pos 24128 vs 24020",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/varm/lib/python3.8/site-packages/torch/serialization.py:629\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 629\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/varm/lib/python3.8/site-packages/torch/serialization.py:863\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    862\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 863\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:769] . PytorchStreamWriter failed writing file data/1: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 92\u001b[0m\n\u001b[1;32m     79\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     80\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     81\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     87\u001b[0m )\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# ===========\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# 7) Train and evaluate\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# ===========\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal test results:\u001b[39m\u001b[38;5;124m\"\u001b[39m, trainer\u001b[38;5;241m.\u001b[39mevaluate(dataset_tok[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "File \u001b[0;32m~/miniconda3/envs/varm/lib/python3.8/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/varm/lib/python3.8/site-packages/transformers/trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2548\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2550\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/miniconda3/envs/varm/lib/python3.8/site-packages/transformers/trainer.py:3007\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   3004\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(trial, ignore_keys_for_eval)\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 3007\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3008\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/miniconda3/envs/varm/lib/python3.8/site-packages/transformers/trainer.py:3101\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   3097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(output_dir, _internal_call\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   3099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[1;32m   3100\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[0;32m-> 3101\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_optimizer_and_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3102\u001b[0m     \u001b[38;5;66;03m# Save RNG state\u001b[39;00m\n\u001b[1;32m   3103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_rng_state(output_dir)\n",
      "File \u001b[0;32m~/miniconda3/envs/varm/lib/python3.8/site-packages/transformers/trainer.py:3247\u001b[0m, in \u001b[0;36mTrainer._save_optimizer_and_scheduler\u001b[0;34m(self, output_dir)\u001b[0m\n\u001b[1;32m   3242\u001b[0m     save_fsdp_optimizer(\n\u001b[1;32m   3243\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfsdp_plugin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, output_dir\n\u001b[1;32m   3244\u001b[0m     )\n\u001b[1;32m   3245\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m   3246\u001b[0m     \u001b[38;5;66;03m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[39;00m\n\u001b[0;32m-> 3247\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOPTIMIZER_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3249\u001b[0m \u001b[38;5;66;03m# Save SCHEDULER & SCALER\u001b[39;00m\n\u001b[1;32m   3250\u001b[0m is_deepspeed_custom_scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_deepspeed_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   3251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler, DeepSpeedSchedulerWrapper\n\u001b[1;32m   3252\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/varm/lib/python3.8/site-packages/torch/serialization.py:630\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    629\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[0;32m--> 630\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n",
      "File \u001b[0;32m~/miniconda3/envs/varm/lib/python3.8/site-packages/torch/serialization.py:476\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 476\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:595] . unexpected pos 24128 vs 24020"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ===========\n",
    "# 2) encode labels (string -> int)\n",
    "# ===========\n",
    "le = LabelEncoder()\n",
    "le.fit(train_df[\"label\"])\n",
    "\n",
    "train_df[\"label_id\"] = le.transform(train_df[\"label\"])\n",
    "val_df[\"label_id\"]   = le.transform(val_df[\"label\"])\n",
    "test_df[\"label_id\"]  = le.transform(test_df[\"label\"])\n",
    "\n",
    "id2label = {i: l for i, l in enumerate(le.classes_)}\n",
    "label2id = {l: i for i, l in id2label.items()}\n",
    "\n",
    "print(\"Classes:\", id2label)\n",
    "\n",
    "\n",
    "# ===========\n",
    "# 3) Convert pandas -> Hugging Face Dataset\n",
    "# ===========\n",
    "train_ds = Dataset.from_pandas(train_df[[\"text\",\"label_id\"]].rename(columns={\"label_id\":\"labels\"}))\n",
    "val_ds   = Dataset.from_pandas(val_df[[\"text\",\"label_id\"]].rename(columns={\"label_id\":\"labels\"}))\n",
    "test_ds  = Dataset.from_pandas(test_df[[\"text\",\"label_id\"]].rename(columns={\"label_id\":\"labels\"}))\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_ds,\n",
    "    \"validation\": val_ds,\n",
    "    \"test\": test_ds\n",
    "})\n",
    "\n",
    "# ===========\n",
    "# 4) Tokenizer & model\n",
    "# ===========\n",
    "model_name = \"distilbert-base-multilingual-cased\"\n",
    "#model_name = \"xlm-roberta-base\"\n",
    "#model_name = \"meta-llama/Llama-3.2-1B\" #\"cardiffnlp/twitter-xlm-roberta-base-emotion\"\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tok(batch[\"text\"], truncation=True, max_length=64) #128\n",
    "\n",
    "dataset_tok = dataset.map(tokenize, batched=True)\n",
    "collator = DataCollatorWithPadding(tokenizer=tok)\n",
    "\n",
    "cfg = AutoConfig.from_pretrained(model_name, num_labels=len(le.classes_), id2label=id2label, label2id=label2id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, config=cfg)\n",
    "\n",
    "# ===========\n",
    "# 5) Metrics\n",
    "# ===========\n",
    "acc = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": acc.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"f1_macro\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "# ===========\n",
    "# 6) Trainer\n",
    "# ===========\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"mdistilbert_emotions\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,#5e-5,\n",
    "    per_device_train_batch_size=8,#16,\n",
    "    per_device_eval_batch_size=16,#32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset_tok[\"train\"],\n",
    "    eval_dataset=dataset_tok[\"validation\"],\n",
    "    tokenizer=tok,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# ===========\n",
    "# 7) Train and evaluate\n",
    "# ===========\n",
    "trainer.train()\n",
    "print(\"Final test results:\", trainer.evaluate(dataset_tok[\"test\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8271ed3",
   "metadata": {},
   "source": [
    "{'loss': 24.9592, 'grad_norm': 1.0552517175674438, 'learning_rate': 0.048660235798499464, 'epoch': 0.08}\n",
    "{'loss': 1.5166, 'grad_norm': 0.22348372638225555, 'learning_rate': 0.04732047159699893, 'epoch': 0.16}\n",
    "{'loss': 1.3658, 'grad_norm': 0.3004882335662842, 'learning_rate': 0.045980707395498394, 'epoch': 0.24}\n",
    "{'loss': 1.3918, 'grad_norm': 0.25394704937934875, 'learning_rate': 0.04464094319399786, 'epoch': 0.32}\n",
    "{'loss': 1.3852, 'grad_norm': 0.3115262985229492, 'learning_rate': 0.04330117899249732, 'epoch': 0.4}\n",
    "{'loss': 1.3673, 'grad_norm': 0.3080769181251526, 'learning_rate': 0.04196141479099679, 'epoch': 0.48}\n",
    "{'loss': 1.3941, 'grad_norm': 0.1880466192960739, 'learning_rate': 0.04062165058949625, 'epoch': 0.56}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67213196",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "varm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
