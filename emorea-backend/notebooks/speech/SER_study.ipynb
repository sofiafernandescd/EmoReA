{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Speech Emotion Recognition (SER) \n",
    "\n",
    "Feature Extraction: \n",
    "- GeMAPS (OpenSMILE)\n",
    "- Full acoustic feature set (librosa)\n",
    "\n",
    "Feature Selection: \n",
    "- Algorithm 1\n",
    "- Algorithm 2\n",
    "- Algorithm 1 + 2\n",
    "\n",
    "Modeling Approach: \n",
    "**Traditional ML:** \n",
    "- Support Vector Machine (SVM) with Radial Basis Function (RBF) kernel \n",
    "- Random Forest \n",
    "- XGBoost (Extreme Gradient Boosting) \n",
    "- Gaussian Naive Bayes \n",
    "- k-Nearest Neighbors (k-NN) \n",
    "**Traditional DL:** \n",
    "- Convolutional Neural Network (CNN) with 2-3 convolutional layers, max pooling, and fully connected layers \n",
    "- Recurrent Neural Network (RNN) with LSTM (Long Short-Term Memory) or GRU (Gated Recurrent Unit) layers \n",
    "- Hybrid CNN-RNN \n",
    "\n",
    "\n",
    "Evaluation Strategy: 10-fold cross validation?? (GridSearchCV??) - less on more expensive tasks (FER/MER)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Total combinations for SER: 2 * 2 * 2 * 2 = 16 experiments \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sofiafernandes/miniconda3/envs/varm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.12)\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/samarwarsi/cmu-mosei?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 27.9G/29.1G [14:14<00:34, 35.1MB/s]  \n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, module_path)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_cmu_mosi\n\u001b[0;32m----> 9\u001b[0m cmu_mosei \u001b[38;5;241m=\u001b[39m \u001b[43mload_cmu_mosi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Repos/EmoReA/emorea-backend/src/utils.py:175\u001b[0m, in \u001b[0;36mload_cmu_mosi\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03mThe CMU-MOSEI (CMU Multimodal Opinion Sentiment and Emotion Intensity) dataset is a large-scale dataset for multimodal sentiment analysis and emotion recognition. \u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03mIt contains video clips of people expressing various emotions, along with corresponding text transcriptions and audio recordings.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m    pd.DataFrame: DataFrame with columns ['filename', 'label']\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# Download dataset from Kaggle\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[43mkagglehub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_download\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msamarwarsi/cmu-mosei\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath to dataset files:\u001b[39m\u001b[38;5;124m\"\u001b[39m, path)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# List all files in the dataset\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/varm/lib/python3.8/site-packages/kagglehub/datasets.py:28\u001b[0m, in \u001b[0;36mdataset_download\u001b[0;34m(handle, path, force_download)\u001b[0m\n\u001b[1;32m     26\u001b[0m h \u001b[38;5;241m=\u001b[39m parse_dataset_handle(handle)\n\u001b[1;32m     27\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading Dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;241m.\u001b[39mto_url()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m, extra\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mEXTRA_CONSOLE_BLOCK})\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_resolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/varm/lib/python3.8/site-packages/kagglehub/registry.py:23\u001b[0m, in \u001b[0;36mMultiImplRegistry.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m impl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impls):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mis_supported(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 23\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m         fails\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mtype\u001b[39m(impl)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/varm/lib/python3.8/site-packages/kagglehub/http_resolver.py:60\u001b[0m, in \u001b[0;36mDatasetHttpResolver.__call__\u001b[0;34m(self, h, path, force_download)\u001b[0m\n\u001b[1;32m     57\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(archive_path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# First, we download the archive.\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marchive_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Create the directory to extract the archive to.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(out_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/varm/lib/python3.8/site-packages/kagglehub/clients.py:167\u001b[0m, in \u001b[0;36mKaggleApiV1Client.download_file\u001b[0;34m(self, path, out_file, resource_handle)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 167\u001b[0m     \u001b[43m_download_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize_read\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_object\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hash_object:\n\u001b[1;32m    170\u001b[0m     actual_md5_hash \u001b[38;5;241m=\u001b[39m to_b64_digest(hash_object)\n",
      "File \u001b[0;32m~/miniconda3/envs/varm/lib/python3.8/site-packages/kagglehub/clients.py:203\u001b[0m, in \u001b[0;36m_download_file\u001b[0;34m(response, out_file, size_read, total_size, hash_object)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(out_file, open_mode) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_content(CHUNK_SIZE):\n\u001b[0;32m--> 203\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m hash_object:\n\u001b[1;32m    205\u001b[0m             hash_object\u001b[38;5;241m.\u001b[39mupdate(chunk)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..', '..')) # or the path to your source code\n",
    "sys.path.insert(0, module_path)\n",
    "\n",
    "from src.utils import load_cmu_mosi\n",
    "\n",
    "cmu_mosei = load_cmu_mosi()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It sounds like you're diving into a fascinating and complex area! Let's break down the input choices for your Deep Learning experiments in Speech Emotion Recognition (SER). You're right to pause and consider this carefully, as the input features significantly impact the performance of your neural networks.\n",
    "\n",
    "## Understanding the Input Options: GeMAPS vs. Full Acoustic Feature Set (including MFCCs)\n",
    "\n",
    "To make the right decision, let's understand what each feature set offers:\n",
    "\n",
    "**1. GeMAPS (Geneva Minimalistic Acoustic Parameter Set):**\n",
    "\n",
    "* **What it is:** GeMAPS is a carefully selected set of 88 acoustic features designed to capture essential paralinguistic information related to emotion. It's based on extensive research in affective computing and aims for a compact yet informative representation of the speech signal.\n",
    "* **Characteristics:**\n",
    "    * **Low dimensionality:** 88 features per time frame. This can lead to faster training and potentially require less data for the models to generalize.\n",
    "    * **Emotionally relevant:** The features are specifically chosen for their known correlation with emotional states.\n",
    "    * **Interpretability:** Some of the features have relatively clear psychoacoustic interpretations (e.g., fundamental frequency, jitter, shimmer, energy-related features, spectral slope).\n",
    "* **When to consider it for DL:**\n",
    "    * **Smaller datasets:** The lower dimensionality might be advantageous when you have a limited amount of training data, as it reduces the risk of overfitting.\n",
    "    * **Focus on paralinguistic cues:** If you believe that the emotional content is primarily conveyed through these higher-level acoustic characteristics.\n",
    "    * **Faster experimentation:** The smaller input size can lead to quicker training times, allowing for more rapid iteration.\n",
    "\n",
    "**2. Full Acoustic Feature Set (librosa - often including MFCCs):**\n",
    "\n",
    "* **What it is:** This typically involves extracting a wide range of acoustic features using a library like `librosa`. A very common and powerful component of this set is Mel-Frequency Cepstral Coefficients (MFCCs).\n",
    "* **Mel-Frequency Cepstral Coefficients (MFCCs):**\n",
    "    * **What they are:** MFCCs are a compact representation of the spectral envelope of a speech signal, mimicking the human auditory system's perception of frequencies. They are derived by taking the Fourier transform of short segments of the audio, mapping the powers of these frequencies onto the mel scale (a perceptual scale of pitches), taking the logarithm of these mel-scaled powers, and then taking the discrete cosine transform (DCT) of the list of mel log powers.\n",
    "    * **Characteristics:** MFCCs capture the short-term power spectrum of the sound, which is highly relevant for phonetic content and also carries emotional information. The number of MFCCs typically ranges from 13 to 40.\n",
    "* **Other features in a \"full\" set (from librosa):** Besides MFCCs, `librosa` can extract a plethora of other features, including:\n",
    "    * **Spectral features:** Spectral centroid, spectral bandwidth, spectral contrast, spectral flatness, spectral roll-off.\n",
    "    * **Temporal features:** Zero-crossing rate, root-mean-square (RMS) energy.\n",
    "    * **Chromagram:** Representation of the spectral energy distribution over 12 pitch classes.\n",
    "    * **And many more.**\n",
    "* **Characteristics of the full set:**\n",
    "    * **High dimensionality:** Combining MFCCs (e.g., 20-40 coefficients per frame) with other spectral and temporal features can result in a significantly higher-dimensional input compared to GeMAPS.\n",
    "    * **Rich information:** This set captures a broader range of acoustic characteristics, potentially including subtle cues that GeMAPS might miss.\n",
    "    * **Potentially better performance (with enough data):** With sufficient training data, the richer input can allow deep learning models to learn more complex patterns and achieve higher accuracy.\n",
    "* **When to consider it for DL:**\n",
    "    * **Larger datasets:** Deep learning models thrive on large amounts of data to learn complex relationships in high-dimensional spaces.\n",
    "    * **Potential for higher accuracy:** The comprehensive feature set might capture more nuanced emotional information.\n",
    "    * **Flexibility for the model:** The neural network can learn to weigh the importance of different features within the set.\n",
    "\n",
    "## Input for Your Deep Learning Architectures: Detailed Explanation\n",
    "\n",
    "Now, let's specifically address how you would use these features as input for your CNN, RNN, and Hybrid models:\n",
    "\n",
    "**1. Convolutional Neural Networks (CNNs):**\n",
    "\n",
    "* **Input format:** CNNs are typically designed to process grid-like data. For audio, this often translates to a 2D representation of the features over time.\n",
    "* **How to prepare the input:**\n",
    "    * **Frame-based extraction:** Extract your chosen feature set (GeMAPS or the full set) for short, overlapping frames of the audio signal. This will give you a sequence of feature vectors over time.\n",
    "    * **Creating the 2D input:** Stack these feature vectors as rows to form a 2D matrix where one dimension represents time frames and the other dimension represents the acoustic features.\n",
    "    * **Example:** If you use MFCCs with 20 coefficients and your audio segment is processed into 100 time frames, your input for one audio segment would be a matrix of shape (100, 20).\n",
    "    * **Channel dimension:** For CNNs, you often need to add a channel dimension. So, your input shape would become (100, 20, 1). If you were to use something like a spectrogram (which is inherently 2D - frequency x time), you might have multiple channels (e.g., magnitude and phase). However, with MFCCs or GeMAPS, you'll likely start with a single channel.\n",
    "* **Considerations:**\n",
    "    * **Input shape consistency:** Ensure all your input sequences have the same length or use techniques like padding or truncation to handle variable lengths.\n",
    "    * **Normalization/Standardization:** It's crucial to normalize or standardize your features (e.g., using StandardScaler from scikit-learn) before feeding them into the neural network to improve training stability and performance.\n",
    "\n",
    "**2. Recurrent Neural Networks (RNNs - LSTMs/GRUs):**\n",
    "\n",
    "* **Input format:** RNNs are designed to process sequential data.\n",
    "* **How to prepare the input:**\n",
    "    * **Frame-based extraction:** Similar to CNNs, extract your chosen feature set for each time frame.\n",
    "    * **Sequential input:** The input for each audio segment will be a sequence of feature vectors.\n",
    "    * **Example:** If you use GeMAPS (88 features) and your audio segment has 150 time frames, your input shape for one segment would be (150, 88).\n",
    "* **Considerations:**\n",
    "    * **Variable sequence lengths:** RNNs can naturally handle variable-length sequences. However, for batch processing, you'll typically need to pad shorter sequences to the length of the longest sequence in the batch. Masking layers can be used to ignore the padded parts.\n",
    "    * **Normalization/Standardization:** Again, normalizing or standardizing your features is essential.\n",
    "\n",
    "**3. Hybrid CNN-RNN:**\n",
    "\n",
    "* **Input format:** These models typically leverage the strengths of both CNNs for feature extraction and RNNs for sequential modeling.\n",
    "* **How to prepare the input:**\n",
    "    * **CNN part:** The initial input to the CNN part would be similar to the CNN case – a 2D representation of your chosen features over time (e.g., (time frames, features, channels)).\n",
    "    * **Feature maps to sequence:** The CNN layers would learn to extract higher-level spatio-temporal features. The output of the CNN (after flattening or using techniques like global pooling) would then be reshaped into a sequential form to be fed into the RNN layers (e.g., a sequence of feature vectors where each vector represents a summary of a local time-frequency region).\n",
    "    * **RNN processing:** The RNN (LSTM or GRU) would then process this sequence of CNN-extracted features to model the temporal dependencies for emotion recognition.\n",
    "* **Considerations:**\n",
    "    * **Careful design of the CNN output to sequence mapping:** How you transition from the CNN's feature maps to the RNN's input sequence is a crucial design choice.\n",
    "    * **Normalization/Standardization:** Apply normalization/standardization to the initial input features.\n",
    "\n",
    "## Which Input Should You Choose for DL Experiments?\n",
    "\n",
    "There's no single \"best\" answer, and it often depends on your dataset size, the complexity of the emotional cues in your data, and the specific architecture you're using. However, here's a general guideline and some suggestions for your experiments:\n",
    "\n",
    "* **Start with MFCCs (as part of the full acoustic set):** MFCCs have a strong track record in speech-related tasks, including emotion recognition. They capture crucial information about the spectral envelope that is relevant to both phonetic content and emotional expression.\n",
    "* **Experiment with the full acoustic feature set from `librosa`:** This allows the model to potentially learn from a wider range of acoustic cues. Be mindful of the increased dimensionality and the potential need for more data.\n",
    "* **Consider GeMAPS as a baseline or for smaller datasets:** If your dataset is relatively small, GeMAPS' lower dimensionality might help prevent overfitting and provide a good starting point.\n",
    "* **Don't be afraid to combine:** You could even explore combining GeMAPS with other features (though this wasn't explicitly in your initial plan).\n",
    "\n",
    "**Recommendations for your experiments:**\n",
    "\n",
    "Given your setup, I would recommend the following initial DL experiments:\n",
    "\n",
    "1.  **CNN with MFCCs:**\n",
    "    * Input: 2D representation of MFCCs (time frames x MFCC coefficients x 1 channel).\n",
    "    * Rationale: MFCCs are a strong baseline, and CNNs excel at learning local patterns in grid-like data, which can be effective for analyzing the spectral evolution over time.\n",
    "\n",
    "2.  **RNN (LSTM or GRU) with MFCCs:**\n",
    "    * Input: Sequence of MFCC vectors (time frames x MFCC coefficients).\n",
    "    * Rationale: RNNs are designed to capture temporal dependencies in sequential data, which is crucial for understanding the evolution of emotional expression in speech.\n",
    "\n",
    "3.  **Hybrid CNN-RNN with MFCCs:**\n",
    "    * Input: 2D representation of MFCCs.\n",
    "    * Rationale: This architecture can leverage the CNN to extract meaningful local features from the MFCCs and the RNN to model the temporal relationships between these features.\n",
    "\n",
    "After exploring these with MFCCs, you can then venture into using the full acoustic feature set from `librosa` with these same architectures to see if the richer input leads to improved performance. Finally, you could try GeMAPS, especially if you find that your dataset is limited.\n",
    "\n",
    "## Evaluation Strategy: 10-Fold Cross-Validation and GridSearchCV\n",
    "\n",
    "Your initial thought of using 10-fold cross-validation is a standard and good practice for evaluating the generalization performance of your models.\n",
    "\n",
    "**GridSearchCV:**\n",
    "\n",
    "* **Purpose:** GridSearchCV is a technique for hyperparameter tuning. It systematically searches through a predefined set of hyperparameter combinations for your chosen model and evaluates the performance of each combination using cross-validation on your training data.\n",
    "* **Integration with 10-fold CV:** You would typically use 10-fold cross-validation *within* each hyperparameter combination being evaluated by GridSearchCV. This ensures a robust estimate of the model's performance for each set of hyperparameters.\n",
    "* **Considerations for DL:** GridSearchCV can be computationally expensive, especially for deep learning models with a large number of hyperparameters and long training times. You might consider:\n",
    "    * **RandomizedSearchCV:** This is a less exhaustive but often more efficient alternative that samples a fixed number of hyperparameter combinations.\n",
    "    * **More focused hyperparameter ranges:** Based on literature or initial pilot experiments, narrow down the ranges of hyperparameters you want to search.\n",
    "    * **Early stopping:** Implement early stopping during the training within each cross-validation fold to save time.\n",
    "\n",
    "**Less on More Expensive Tasks (FER/MER):**\n",
    "\n",
    "Your intuition about reducing the cross-validation folds or the granularity of the hyperparameter search for more computationally expensive tasks like Facial Emotion Recognition (FER) or Music Emotion Recognition (MER) is correct. These tasks often involve larger input dimensions (e.g., images or longer audio sequences) and more complex models, leading to significantly longer training times. In such cases, you might consider:\n",
    "\n",
    "* **Fewer cross-validation folds (e.g., 3 or 5).**\n",
    "* **A coarser grid of hyperparameters in GridSearchCV or using RandomizedSearchCV.**\n",
    "* **Focusing on tuning the most critical hyperparameters.**\n",
    "\n",
    "For your SER experiments, starting with 10-fold cross-validation combined with GridSearchCV (if computationally feasible) is a good approach to thoroughly evaluate your models and find optimal hyperparameters.\n",
    "\n",
    "In summary, for your deep learning SER experiments, begin by focusing on **MFCCs** as input for your CNN, RNN, and hybrid models. Experiment with the 2D representation for CNNs and the sequential representation for RNNs. Remember to normalize your features and consider the impact of input dimensionality on your model complexity and data requirements. Good luck with your research!\n",
    "\n",
    "1.  **Data Loading and Preprocessing:**\n",
    "    * **Load Audio Files:** Use libraries like `librosa` to load your audio files. This will typically give you a 1-dimensional NumPy array representing the audio waveform and the sampling rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def load_audio(file_path, target_sr=16000):\n",
    "    y, sr = librosa.load(file_path, sr=target_sr)\n",
    "    return y, sr\n",
    "\n",
    "audio_path = 'path/to/your/audio.wav'\n",
    "audio_signal, sampling_rate = load_audio(audio_path)\n",
    "print(f\"Audio shape: {audio_signal.shape}\")\n",
    "print(f\"Sampling rate: {sampling_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach aims to let the network learn the relevant features directly from the waveform, potentially bypassing the need for handcrafted feature extraction like MFCCs or GeMAPS.\n",
    "\n",
    "* **Resampling (Optional but Recommended):** Standardize the sampling rate across your dataset. This ensures consistent input dimensions for your network. Choose a common sampling rate (e.g., 16000 Hz).\n",
    "    * **Normalization:** Normalize the audio signal to a consistent range (e.g., between -1 and 1). This helps with training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_audio(audio):\n",
    "    return audio / np.max(np.abs(audio))\n",
    "\n",
    "normalized_signal = normalize_audio(audio_signal)\n",
    "print(f\"Normalized audio range: {np.min(normalized_signal):.4f} to {np.max(normalized_signal):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Padding or Truncation:** Audio files will likely have variable lengths. Neural networks typically require fixed-size input. You'll need to either:\n",
    "    * **Pad:** Add zeros to the end of shorter audio signals to match the length of the longest signal (or a predefined maximum length).\n",
    "    * **Truncate:** Cut off longer audio signals to a predefined maximum length.\n",
    "    * **Segmentation:** Split longer audio into fixed-length segments. This can also increase your training data.\n",
    "\n",
    "* **Creating Batches:** When training, you'll feed data to the network in batches. Organize your processed audio signals and their corresponding labels into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate(audio, target_length):\n",
    "    current_length = len(audio)\n",
    "    if current_length < target_length:\n",
    "        padding = np.zeros(target_length - current_length)\n",
    "        return np.concatenate((audio, padding))\n",
    "    elif current_length > target_length:\n",
    "        return audio[:target_length]\n",
    "    else:\n",
    "        return audio\n",
    "\n",
    "target_length_samples = int(2 * sampling_rate) # Example: 2 seconds at 16kHz\n",
    "processed_signal = pad_or_truncate(normalized_signal, target_length_samples)\n",
    "print(f\"Processed audio shape: {processed_signal.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  **Neural Network Architecture:**\n",
    "    * **1D Convolutional Neural Networks (1D CNNs):** These are the most common architecture for processing raw audio. 1D convolutional layers can learn temporal patterns directly from the waveform.\n",
    "    * **Input Layer:** The input layer of your network will have a shape corresponding to the fixed length of your processed audio signals (e.g., `(target_length_samples, 1)` if you consider it a single channel).\n",
    "    * **1D Convolutional Layers:** These layers will slide 1D filters across the time dimension of the audio, learning local temporal features. You'll typically have multiple convolutional layers with increasing numbers of filters to capture increasingly complex patterns.\n",
    "    * **Activation Functions:** Use non-linear activation functions (e.g., ReLU) after each convolutional layer.\n",
    "    * **Pooling Layers (e.g., MaxPooling1D):** Downsample the temporal dimension, reducing the number of parameters and increasing the receptive field of subsequent layers.\n",
    "    * **Flatten Layer:** Flatten the output of the convolutional/pooling layers into a 1D vector before feeding it into fully connected layers.\n",
    "    * **Fully Connected (Dense) Layers:** These layers will learn the final mapping from the learned features to the emotion classes.\n",
    "    * **Output Layer:** A dense layer with the number of units equal to the number of emotion classes and a softmax activation function for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def create_raw_audio_cnn_model(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Conv1D(32, kernel_size=5, activation='relu', padding='same'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Conv1D(64, kernel_size=3, activation='relu', padding='same'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "target_length = int(2 * 16000)\n",
    "num_emotions = 8 # Example\n",
    "input_shape = (target_length, 1) # Single channel for raw audio\n",
    "\n",
    "raw_audio_model = create_raw_audio_cnn_model(input_shape, num_emotions)\n",
    "raw_audio_model.summary()\n",
    "\n",
    "raw_audio_model.compile(optimizer='adam',\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "# Assuming you have your training data (X_train, y_train) prepared\n",
    "# X_train should have shape (num_samples, target_length) and needs a channel dimension\n",
    "# X_train = np.expand_dims(X_train, axis=-1)\n",
    "# raw_audio_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.  **Training and Evaluation:**\n",
    "    * Train your model using your prepared data and labels.\n",
    "    * Use an appropriate loss function (e.g., `sparse_categorical_crossentropy` for integer labels or `categorical_crossentropy` for one-hot encoded labels) and an optimizer (e.g., Adam).\n",
    "    * Evaluate the model's performance on your test set using appropriate metrics (e.g., accuracy, F1-score).\n",
    "    * Use your 10-fold cross-validation strategy (potentially with GridSearchCV for hyperparameter tuning) as you planned.\n",
    "\n",
    "**Advantages of Using Raw Audio Input:**\n",
    "\n",
    "* **End-to-end learning:** The network learns features directly tailored to the task, potentially capturing subtle nuances that handcrafted features might miss.\n",
    "* **Reduced reliance on domain expertise:** You don't need to manually design features based on acoustic knowledge.\n",
    "* **Potential for capturing non-linear relationships:** Deep neural networks can learn complex, non-linear relationships in the raw waveform.\n",
    "\n",
    "**Challenges and Considerations:**\n",
    "\n",
    "* **High dimensionality:** Raw audio signals have a very high dimensionality in the time domain. For example, a 1-second audio clip at 16 kHz has 16,000 data points. This can lead to:\n",
    "    * **Increased computational cost:** Training can be much slower and require more memory.\n",
    "    * **Larger number of parameters:** The network might need more parameters to learn from the high-dimensional input, potentially requiring more training data to avoid overfitting.\n",
    "* **Sensitivity to irrelevant variations:** Raw audio can contain variations (e.g., background noise, speaker characteristics) that are not directly related to emotion. The network might learn to focus on these irrelevant details if not trained carefully with sufficient data and regularization.\n",
    "* **Long-range dependencies:** Capturing long-range temporal dependencies in the raw waveform can be challenging for simple CNNs. You might need very deep networks or combine them with recurrent layers.\n",
    "* **Need for substantial data:** Training deep networks on raw audio effectively often requires a significantly larger dataset compared to using well-engineered features.\n",
    "\n",
    "**When might raw audio input be beneficial?**\n",
    "\n",
    "* **Very large datasets:** If you have a massive amount of training data, the network has more opportunities to learn meaningful features.\n",
    "* **Tasks where handcrafted features might be limiting:** In cases where the emotional cues are very subtle or complex and not well-captured by traditional features.\n",
    "* **Research settings:** For exploring the capabilities of deep learning to automatically learn representations from raw sensory data.\n",
    "\n",
    "**In your case:**\n",
    "\n",
    "Given that you are also exploring traditional ML with handcrafted features, it would be a valuable experiment to try a 1D CNN with raw audio input as a comparison. Start with a relatively shallow CNN architecture and see how it performs. Be prepared for potentially longer training times and the need for careful data preprocessing (especially handling variable lengths).\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "Yes, you can definitely input raw audio into a neural network, primarily using 1D CNNs. However, be aware of the challenges related to high dimensionality and the potential need for large datasets and careful model design. It's a different paradigm compared to using handcrafted features, and the results might vary depending on your specific dataset and the complexity of the emotional cues. Experimentation is key!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "varm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
