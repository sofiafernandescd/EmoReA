# EmoReA
## emorea-frontend

## emorea-backend


# TFM

- acoustic-eda: audioFeatureSelector (70% EmoDB)
- app_MER (begginings of app)


# TFM-SC

- app.py: 
    -  https://medium.com/@codeaigo/building-a-speech-emotion-recognition-app-with-ai-understanding-emotions-in-real-time-46d42401bfdc
    - https://thiagoalves.ai/building-webcam-streaming-applications-with-streamlit-and-opencv/
 
 - FER-MVP.ipynb: FER using Pytorch and Tensorflow (and also some audio stuff)

 - finetune-bert.ipynb: text experiments
 
- MER-mvp.ipynb: In fact FER approaches

- MultimodalEmotionRecognition-EDA.ipynb:
    - Transcript extraction whisper (tried speach_recognition)
    - Split audio by sentences
    - Audio EDA (SUSAS)
    - EmoDB EDA
    - Solutions for audio (62%) and text (24%)

## notebooks
- 01-ravdess-feature-selection.ipynb: 
    - EDA RAVDESS
    - OpenSMILE GeMAPS
    - Feature selection comparing (59% SVM, 65% RF)
    - Joint RAVDESS+EMODB EDA (58% vs 53% selected)
    - CNN did not train
- 02-emodb-feature-selection
    - EDA and SVM 71% with boredom
    - 76% with FS
- 03-ravdess-emodb-feature-selection
- 04-interspeech-feature-selection
- 05-fer: CLIPModel
- emotion-recognition-on-multimedia-content: good audio EDA


