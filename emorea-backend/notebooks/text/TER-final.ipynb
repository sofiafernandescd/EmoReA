{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3182a44",
   "metadata": {},
   "source": [
    "# TER Experiments\n",
    "## Expriment 1: Whisper Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6055a124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ddad3b5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Expriment 2: Train and Fine-Tune (Isolated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf056a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6be6faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import gensim.downloader as api\n",
    "\n",
    "# -----------------\n",
    "# Embedding Transformers\n",
    "# -----------------\n",
    "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, size=100, window=5, min_count=1):\n",
    "        self.size = size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        tokenized = [t.split() for t in X]\n",
    "        self.model = Word2Vec(sentences=tokenized, vector_size=self.size,\n",
    "                              window=self.window, min_count=self.min_count, workers=4)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        tokenized = [t.split() for t in X]\n",
    "        features = []\n",
    "        for tokens in tokenized:\n",
    "            vecs = [self.model.wv[w] for w in tokens if w in self.model.wv]\n",
    "            if len(vecs) > 0:\n",
    "                features.append(np.mean(vecs, axis=0))\n",
    "            else:\n",
    "                features.append(np.zeros(self.size))\n",
    "        return np.array(features)\n",
    "\n",
    "\n",
    "class GloVeTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, size=100):\n",
    "        self.size = size\n",
    "        self.glove = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # load pretrained GloVe from gensim (small version)\n",
    "        self.glove = api.load(f'glove-wiki-gigaword-{self.size}')\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        tokenized = [t.split() for t in X]\n",
    "        features = []\n",
    "        for tokens in tokenized:\n",
    "            vecs = [self.glove[w] for w in tokens if w in self.glove]\n",
    "            if len(vecs) > 0:\n",
    "                features.append(np.mean(vecs, axis=0))\n",
    "            else:\n",
    "                features.append(np.zeros(self.size))\n",
    "        return np.array(features)\n",
    "\n",
    "\n",
    "# -----------------\n",
    "# Experiment Function\n",
    "# -----------------\n",
    "def run_experiment(df):\n",
    "    X = df['text']\n",
    "    y = df['label']\n",
    "\n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    classifiers = {\n",
    "        'SVM': (SVC(), {'clf__C': [0.1, 1, 10], 'clf__kernel': ['linear', 'rbf']}),\n",
    "        'RandomForest': (RandomForestClassifier(), {'clf__n_estimators': [50, 100]}),\n",
    "        'NaiveBayes': (GaussianNB(), {}),\n",
    "        'LogReg': (LogisticRegression(max_iter=500), {'clf__C': [0.1, 1, 10]})\n",
    "    }\n",
    "\n",
    "    embeddings = {\n",
    "        'Word2Vec': Word2VecTransformer(size=100),\n",
    "        'GloVe': GloVeTransformer(size=100)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for emb_name, emb in embeddings.items():\n",
    "        for clf_name, (clf, params) in classifiers.items():\n",
    "            pipe = Pipeline([\n",
    "                ('emb', emb),\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('clf', clf)\n",
    "            ])\n",
    "\n",
    "            grid = GridSearchCV(pipe, param_grid=params, cv=loo, scoring='accuracy', n_jobs=-1)\n",
    "            grid.fit(X, y)\n",
    "\n",
    "            results[(emb_name, clf_name)] = grid.best_score_\n",
    "\n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7262e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_experiment(df)\n",
    "print(\"Results:\")\n",
    "for k, v in results.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baac2d1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
